{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Web Scraper\n",
    "\n",
    "In this project I create web scarper to get features from amazon products.\n",
    "\n",
    "I use python and some libraries like: BeautifulSoup4, requests, and etc...\n",
    "\n",
    "Before Import this libraries you need to installing--> Use: `Conda install BeautifulSoup4` and `Conda install requests` , consider that you can use pip instead of conda with this codes: `pip install BeautifulSoup4` or `pip install requests` .\n",
    "So let's deep dive in project and enjoy:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import smtplib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny Got Data MIS Data Systems Business Analyst T-Shirt\n",
      "16.99\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_6?keywords=data+analyst+shirt&qid=1650387916&sr=8-6'\n",
    "\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "page = requests.get(URL, headers = headers)\n",
    "\n",
    "soup1 = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "soup2 = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "title = soup2.find(id = 'productTitle').getText(strip = True)\n",
    "price = soup2.find('span', {'class':'a-offscreen'}).getText(strip = True)\n",
    "\n",
    "#strip = True --> because we have a lot of space in ourput and with this statement we can remove them.\n",
    "\n",
    "\n",
    "# Another method for geting the price is:\n",
    "#price = soup2.find(class_='a-offscreen')\n",
    "#price = price.get_text() if price else \"No Description\"\n",
    "'''\n",
    "in each 2 method we have same output and it's $16.99, For removing '$' sign we can use 'replace' method \n",
    "or use the [1:]. I prefer use the 'replace' because it's more reliable. \n",
    "'''\n",
    "\n",
    "price = float(price.replace('$', ''))\n",
    "\n",
    "\n",
    "print(title)\n",
    "print(price)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-20\n"
     ]
    }
   ],
   "source": [
    "# Also I want to add some featurs like date:\n",
    "# we imported datetime \n",
    "\n",
    "today = datetime.date.today()\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's time save out data in a csv file:\n",
    "\n",
    "import csv\n",
    "\n",
    "header = ['Title', 'Price', 'Date']\n",
    "data = [title, price, today]\n",
    "\n",
    "with open ('AmazonScraperDataSet.csv', 'w', newline='', encoding= 'UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Funny Got Data MIS Data Systems Business Analy...</td>\n",
       "      <td>16.99</td>\n",
       "      <td>2022-04-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Price        Date\n",
       "0  Funny Got Data MIS Data Systems Business Analy...  16.99  2022-04-20"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the our data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('AmazonScraperDataSet.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's seems everything is good, but we have an important problem and it's our DataFrame can accept just one value, in this situation if we need expand our dataset what we should do?\n",
    "\n",
    "So, come with me :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append our list\n",
    "\n",
    "with open('AmazonScraperDataSet.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an automation system:\n",
    "\n",
    "def check_price():\n",
    "    URL = 'https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_6?keywords=data+analyst+shirt&qid=1650387916&sr=8-6'\n",
    "\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "    page = requests.get(URL, headers = headers)\n",
    "\n",
    "    soup1 = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    soup2 = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "    title = soup2.find(id = 'productTitle').getText(strip = True)\n",
    "    price = soup2.find('span', {'class':'a-offscreen'}).getText(strip = True)\n",
    "\n",
    "    #strip = True --> because we have a lot of space in ourput and with this statement we can remove them.\n",
    "\n",
    "\n",
    "    # Another method for geting the price is:\n",
    "    #price = soup2.find(class_='a-offscreen')\n",
    "    #price = price.get_text() if price else \"No Description\"\n",
    "    '''\n",
    "    in each 2 method we have same output and it's $16.99, For removing '$' sign we can use 'replace' method \n",
    "    or use the [1:]. I prefer use the 'replace' because it's more reliable. \n",
    "    '''\n",
    "    \n",
    "    price = float(price.replace('$', ''))\n",
    "    \n",
    "    # Also I want to add some featurs like date:\n",
    "    # we imported datetime \n",
    "\n",
    "    today = datetime.date.today()\n",
    "    print(today)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now it's time save out data in a csv file:\n",
    "\n",
    "    import csv\n",
    "\n",
    "    header = ['Title', 'Price', 'Date']\n",
    "    data = [title, price, today]\n",
    "\n",
    "    with open ('AmazonScraperDataSet.csv', 'w', newline='', encoding= 'UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        writer.writerow(data)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with this code (change the time based on sec) --> 86400\n",
    "\n",
    "while(True):\n",
    "    check_price()\n",
    "    time.sleep(86400)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If uou want to try sending yourself an email (just for fun) when a price hits below a certain level \n",
    "#you can try it out with this script\n",
    "\n",
    "if price<15\n",
    "\n",
    "def send_mail():\n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com',465)\n",
    "    server.ehlo()\n",
    "    #server.starttls()\n",
    "    server.ehlo()\n",
    "    server.login('YourMail@gmail.com','YourPassword')\n",
    "    \n",
    "    subject = \"The Shirt you want is below $15! Now is your chance to buy!\"\n",
    "    body = \"Alex, This is the moment we have been waiting for. Now is your chance to pick up the shirt of your dreams. Don't mess it up! Link here: https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_3?dchild=1&keywords=data+analyst+tshirt&qid=1626655184&sr=8-3\"\n",
    "   \n",
    "    msg = f\"Subject: {subject}\\n\\n{body}\"\n",
    "    \n",
    "    server.sendmail(\n",
    "        'AlexTheAnalyst95@gmail.com',\n",
    "        msg\n",
    "     \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
